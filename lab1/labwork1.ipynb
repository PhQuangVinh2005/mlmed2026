{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22510938",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74d336c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kagglehub[pandas-datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0bf663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilovekaguya/miniconda3/envs/uni/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading to /home/ilovekaguya/.cache/kagglehub/datasets/shayanfazeli/heartbeat/1.archive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98.8M/98.8M [00:17<00:00, 6.05MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded to: /home/ilovekaguya/.cache/kagglehub/datasets/shayanfazeli/heartbeat/versions/1\n",
      "Available files:\n",
      "  - ptbdb_abnormal.csv\n",
      "  - ptbdb_normal.csv\n",
      "  - mitbih_test.csv\n",
      "  - mitbih_train.csv\n"
     ]
    }
   ],
   "source": [
    "# import kagglehub\n",
    "# from kagglehub import KaggleDatasetAdapter\n",
    "# import os\n",
    "\n",
    "# path = kagglehub.dataset_download(\"shayanfazeli/heartbeat\")\n",
    "# print(\"Dataset downloaded to:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4614dfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied: ptbdb_abnormal.csv\n",
      "Copied: ptbdb_normal.csv\n",
      "Copied: mitbih_test.csv\n",
      "Copied: mitbih_train.csv\n",
      "\n",
      "All files copied to: /home/ilovekaguya/usth/B3/ml_in_med/mlmed2025/lab1/data\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "source_path = \"/home/ilovekaguya/.cache/kagglehub/datasets/shayanfazeli/heartbeat/versions/1\"\n",
    "dest_path = \"/home/ilovekaguya/usth/B3/ml_in_med/mlmed2025/lab1/data\"\n",
    "\n",
    "os.makedirs(dest_path, exist_ok=True)\n",
    "\n",
    "for file in os.listdir(source_path):\n",
    "    source_file = os.path.join(source_path, file)\n",
    "    dest_file = os.path.join(dest_path, file)\n",
    "    shutil.copy2(source_file, dest_file)\n",
    "    print(f\"Copied: {file}\")\n",
    "\n",
    "print(f\"\\nAll files copied to: {dest_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2fc1bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3c197e8",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8828442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(87554, 188)}\n",
      "{(21892, 188)}\n",
      "\n",
      "Training set class distribution:\n",
      "0    72471\n",
      "1     2223\n",
      "2     5788\n",
      "3      641\n",
      "4     6431\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test set class distribution:\n",
      "0    18118\n",
      "1      556\n",
      "2     1448\n",
      "3      162\n",
      "4     1608\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(\"data/mitbih_train.csv\", header=None)\n",
    "print({train_df.shape})\n",
    "\n",
    "test_df = pd.read_csv(\"data/mitbih_test.csv\", header=None)\n",
    "print({test_df.shape})\n",
    "\n",
    "X_train = train_df.iloc[:, :187].values\n",
    "y_train = train_df.iloc[:, 187].values.astype(int)\n",
    "\n",
    "X_test = test_df.iloc[:, :187].values\n",
    "y_test = test_df.iloc[:, 187].values.astype(int)\n",
    "\n",
    "print(pd.Series(y_train).value_counts().sort_index())\n",
    "print(pd.Series(y_test).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08928514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.977941</td>\n",
       "      <td>0.926471</td>\n",
       "      <td>0.681373</td>\n",
       "      <td>0.245098</td>\n",
       "      <td>0.154412</td>\n",
       "      <td>0.191176</td>\n",
       "      <td>0.151961</td>\n",
       "      <td>0.085784</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.049020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.960114</td>\n",
       "      <td>0.863248</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.196581</td>\n",
       "      <td>0.094017</td>\n",
       "      <td>0.125356</td>\n",
       "      <td>0.099715</td>\n",
       "      <td>0.088319</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.082621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.659459</td>\n",
       "      <td>0.186486</td>\n",
       "      <td>0.070270</td>\n",
       "      <td>0.070270</td>\n",
       "      <td>0.059459</td>\n",
       "      <td>0.056757</td>\n",
       "      <td>0.043243</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.045946</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.925414</td>\n",
       "      <td>0.665746</td>\n",
       "      <td>0.541436</td>\n",
       "      <td>0.276243</td>\n",
       "      <td>0.196133</td>\n",
       "      <td>0.077348</td>\n",
       "      <td>0.071823</td>\n",
       "      <td>0.060773</td>\n",
       "      <td>0.066298</td>\n",
       "      <td>0.058011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.967136</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.830986</td>\n",
       "      <td>0.586854</td>\n",
       "      <td>0.356808</td>\n",
       "      <td>0.248826</td>\n",
       "      <td>0.145540</td>\n",
       "      <td>0.089202</td>\n",
       "      <td>0.117371</td>\n",
       "      <td>0.150235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 188 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.977941  0.926471  0.681373  0.245098  0.154412  0.191176  0.151961   \n",
       "1  0.960114  0.863248  0.461538  0.196581  0.094017  0.125356  0.099715   \n",
       "2  1.000000  0.659459  0.186486  0.070270  0.070270  0.059459  0.056757   \n",
       "3  0.925414  0.665746  0.541436  0.276243  0.196133  0.077348  0.071823   \n",
       "4  0.967136  1.000000  0.830986  0.586854  0.356808  0.248826  0.145540   \n",
       "\n",
       "        7         8         9    ...  178  179  180  181  182  183  184  185  \\\n",
       "0  0.085784  0.058824  0.049020  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1  0.088319  0.074074  0.082621  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2  0.043243  0.054054  0.045946  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3  0.060773  0.066298  0.058011  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4  0.089202  0.117371  0.150235  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   186  187  \n",
       "0  0.0  0.0  \n",
       "1  0.0  0.0  \n",
       "2  0.0  0.0  \n",
       "3  0.0  0.0  \n",
       "4  0.0  0.0  \n",
       "\n",
       "[5 rows x 188 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f41ce2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.758264</td>\n",
       "      <td>0.111570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080579</td>\n",
       "      <td>0.078512</td>\n",
       "      <td>0.066116</td>\n",
       "      <td>0.049587</td>\n",
       "      <td>0.047521</td>\n",
       "      <td>0.035124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.908425</td>\n",
       "      <td>0.783883</td>\n",
       "      <td>0.531136</td>\n",
       "      <td>0.362637</td>\n",
       "      <td>0.366300</td>\n",
       "      <td>0.344322</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.296703</td>\n",
       "      <td>0.300366</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.730088</td>\n",
       "      <td>0.212389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.119469</td>\n",
       "      <td>0.101770</td>\n",
       "      <td>0.101770</td>\n",
       "      <td>0.110619</td>\n",
       "      <td>0.123894</td>\n",
       "      <td>0.115044</td>\n",
       "      <td>0.132743</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.910417</td>\n",
       "      <td>0.681250</td>\n",
       "      <td>0.472917</td>\n",
       "      <td>0.229167</td>\n",
       "      <td>0.068750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004167</td>\n",
       "      <td>0.014583</td>\n",
       "      <td>0.054167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.570470</td>\n",
       "      <td>0.399329</td>\n",
       "      <td>0.238255</td>\n",
       "      <td>0.147651</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003356</td>\n",
       "      <td>0.040268</td>\n",
       "      <td>0.080537</td>\n",
       "      <td>0.070470</td>\n",
       "      <td>0.090604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 188 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  1.000000  0.758264  0.111570  0.000000  0.080579  0.078512  0.066116   \n",
       "1  0.908425  0.783883  0.531136  0.362637  0.366300  0.344322  0.333333   \n",
       "2  0.730088  0.212389  0.000000  0.119469  0.101770  0.101770  0.110619   \n",
       "3  1.000000  0.910417  0.681250  0.472917  0.229167  0.068750  0.000000   \n",
       "4  0.570470  0.399329  0.238255  0.147651  0.000000  0.003356  0.040268   \n",
       "\n",
       "        7         8         9    ...  178  179  180  181  182  183  184  185  \\\n",
       "0  0.049587  0.047521  0.035124  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1  0.307692  0.296703  0.300366  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2  0.123894  0.115044  0.132743  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3  0.004167  0.014583  0.054167  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4  0.080537  0.070470  0.090604  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   186  187  \n",
       "0  0.0  0.0  \n",
       "1  0.0  0.0  \n",
       "2  0.0  0.0  \n",
       "3  0.0  0.0  \n",
       "4  0.0  0.0  \n",
       "\n",
       "[5 rows x 188 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a62054",
   "metadata": {},
   "source": [
    "Number of Samples: 109446 <br>\n",
    "Number of Categories: 5 <br>\n",
    "Sampling Frequency: 125Hz <br>\n",
    "Data Source: Physionet's MIT-BIH Arrhythmia Dataset <br>\n",
    "Classes: ['N': 0, 'S': 1, 'V': 2, 'F': 3, 'Q': 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a17e7475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: figures/01_class_distribution.pdf\n"
     ]
    }
   ],
   "source": [
    "# Visualize class distribution with bar chart\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get class distribution from training data\n",
    "train_class_distribution = pd.Series(y_train).value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "train_class_distribution.plot(kind='bar', color=['#4472C4', '#70AD47', '#FFC000', '#5B9BD5', '#A5A5A5'])\n",
    "plt.title('Frequency of each class in the training dataset', fontsize=12)\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Pie chart\n",
    "plt.subplot(1, 2, 2)\n",
    "colors = ['red', 'green', 'blue', 'skyblue', 'orange']\n",
    "plt.pie(train_class_distribution, labels=['N (0)', 'S (1)', 'V (2)', 'F (3)', 'Q (4)'], \n",
    "        colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "centre_circle = plt.Circle((0, 0), 0.70, fc='white')\n",
    "fig = plt.gcf()\n",
    "\n",
    "fig.gca().add_artist(centre_circle)\n",
    "plt.savefig('figures/01_class_distribution.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"Saved: figures/01_class_distribution.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e0e8d471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12902/4085586286.py:3: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  samples = train_df.groupby(187, group_keys=False).apply(lambda x: x.sample(1, random_state=42))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: figures/02_ecg_signals_per_class.pdf\n"
     ]
    }
   ],
   "source": [
    "# Visualize sample ECG signals from each class\n",
    "# Get one sample from each class from training data\n",
    "samples = train_df.groupby(187, group_keys=False).apply(lambda x: x.sample(1, random_state=42))\n",
    "\n",
    "fig, axes = plt.subplots(5, 1, figsize=(15, 12))\n",
    "class_names = ['Normal (N)', 'Supraventricular (S)', 'Ventricular (V)', 'Fusion (F)', 'Unknown (Q)']\n",
    "\n",
    "for i, (idx, row) in enumerate(samples.iterrows()):\n",
    "    signal = row[:187].values\n",
    "    axes[i].plot(signal, linewidth=1.5, color=['#4472C4', '#70AD47', '#FFC000', '#5B9BD5', '#A5A5A5'][i])\n",
    "    axes[i].set_title(f'Class {i}: {class_names[i]}', fontsize=12, fontweight='bold')\n",
    "    axes[i].set_xlabel('Time (samples)')\n",
    "    axes[i].set_ylabel('Amplitude')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/02_ecg_signals_per_class.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "print(\"Saved: figures/02_ecg_signals_per_class.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "047c851d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical Summary of the Training Dataset:\n",
      "==================================================\n",
      "\n",
      "Class 0 (Normal (N)):\n",
      "  Count: 72471\n",
      "  Mean signal amplitude: 0.1619\n",
      "  Std signal amplitude: 0.2176\n",
      "  Min signal amplitude: 0.0000\n",
      "  Max signal amplitude: 1.0000\n",
      "\n",
      "Class 1 (Supraventricular (S)):\n",
      "  Count: 2223\n",
      "  Mean signal amplitude: 0.1948\n",
      "  Std signal amplitude: 0.2146\n",
      "  Min signal amplitude: 0.0000\n",
      "  Max signal amplitude: 1.0000\n",
      "\n",
      "Class 2 (Ventricular (V)):\n",
      "  Count: 5788\n",
      "  Mean signal amplitude: 0.2438\n",
      "  Std signal amplitude: 0.2623\n",
      "  Min signal amplitude: 0.0000\n",
      "  Max signal amplitude: 1.0000\n",
      "\n",
      "Class 3 (Fusion (F)):\n",
      "  Count: 641\n",
      "  Mean signal amplitude: 0.1073\n",
      "  Std signal amplitude: 0.1930\n",
      "  Min signal amplitude: 0.0000\n",
      "  Max signal amplitude: 1.0000\n",
      "\n",
      "Class 4 (Unknown (Q)):\n",
      "  Count: 6431\n",
      "  Mean signal amplitude: 0.2509\n",
      "  Std signal amplitude: 0.2643\n",
      "  Min signal amplitude: 0.0000\n",
      "  Max signal amplitude: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Statistical summary\n",
    "class_names = ['Normal (N)', 'Supraventricular (S)', 'Ventricular (V)', 'Fusion (F)', 'Unknown (Q)']\n",
    "\n",
    "print(\"Statistical Summary of the Training Dataset:\")\n",
    "print(\"=\"*50)\n",
    "for class_label in range(5):\n",
    "    class_data = train_df[train_df[187] == class_label].iloc[:, :187]\n",
    "    print(f\"\\nClass {class_label} ({class_names[class_label]}):\")\n",
    "    print(f\"  Count: {len(class_data)}\")\n",
    "    print(f\"  Mean signal amplitude: {class_data.values.mean():.4f}\")\n",
    "    print(f\"  Std signal amplitude: {class_data.values.std():.4f}\")\n",
    "\n",
    "    print(f\"  Min signal amplitude: {class_data.values.min():.4f}\")    \n",
    "    print(f\"  Max signal amplitude: {class_data.values.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71b3f30",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da34cd5",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6b1515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   17.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   17.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed in 43.92 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   43.8s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import time\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=42,\n",
    "    random_state=42,\n",
    "    n_jobs=6, # my i5-10th has 8 cores\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Start training\")\n",
    "start_time = time.time()\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nCompleted in {training_time:.2f} secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1c0212ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 97.45%\n",
      "F1 Score (Weighted): 0.9728\n",
      "F1 Score (Macro): 0.8716\n",
      "\n",
      "Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          Normal (N)     0.9728    0.9992    0.9858     18118\n",
      "Supraventricular (S)     0.9796    0.6043    0.7475       556\n",
      "     Ventricular (V)     0.9823    0.8805    0.9286      1448\n",
      "          Fusion (F)     0.8850    0.6173    0.7273       162\n",
      "         Unknown (Q)     0.9941    0.9447    0.9688      1608\n",
      "\n",
      "            accuracy                         0.9745     21892\n",
      "           macro avg     0.9627    0.8092    0.8716     21892\n",
      "        weighted avg     0.9745    0.9745    0.9728     21892\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"\\nTest Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "f1_weighted = f1_score(y_test, y_pred_rf, average='weighted')\n",
    "f1_macro = f1_score(y_test, y_pred_rf, average='macro')\n",
    "print(f\"F1 Score (Weighted): {f1_weighted:.4f}\")\n",
    "print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "class_names = ['Normal (N)', 'Supraventricular (S)', 'Ventricular (V)', 'Fusion (F)', 'Unknown (Q)']\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf, target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea1c83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "Saved: figures/04_rf_confusion_matrix.pdf\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_rf)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "plot_confusion_matrix(cm, classes=['N', 'S', 'V', 'F', 'Q'], normalize=True,\n",
    "                      title='Random Forest - Normalized Confusion Matrix')\n",
    "plt.savefig('figures/04_rf_confusion_matrix.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "print(\"Saved: figures/04_rf_confusion_matrix.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1556e5d0",
   "metadata": {},
   "source": [
    "## 1D Convolutional Neural Network (1D-CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b963f76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "X_train_tensor shape: torch.Size([87554, 1, 187])\n",
      "X_test_tensor shape: torch.Size([21892, 1, 187])\n",
      "y_train_tensor shape: torch.Size([87554])\n",
      "y_test_tensor shape: torch.Size([21892])\n",
      "\n",
      "Number of training batches: 2737\n",
      "Number of test batches: 685\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled).unsqueeze(1)  # Add channel dimension: (N, 1, 187)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled).unsqueeze(1)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "print({X_train_tensor.shape},\n",
    "      {X_test_tensor.shape},\n",
    "      {y_train_tensor.shape},\n",
    "      {y_test_tensor.shape})\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"\\nNumber of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b493873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN1D(\n",
      "  (conv1): Conv1d(1, 64, kernel_size=(6,), stride=(1,), padding=same)\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu1): ReLU()\n",
      "  (pool1): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same)\n",
      "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu2): ReLU()\n",
      "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=same)\n",
      "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu3): ReLU()\n",
      "  (pool3): MaxPool1d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=1600, out_features=64, bias=True)\n",
      "  (relu4): ReLU()\n",
      "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (relu5): ReLU()\n",
      "  (fc3): Linear(in_features=32, out_features=5, bias=True)\n",
      ")\n",
      "\n",
      "Total parameters: 130,245\n"
     ]
    }
   ],
   "source": [
    "# Define 1D CNN model architecture in PyTorch\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(CNN1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=6, padding='same')\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, padding='same')\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, padding='same')\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2, padding=1)\n",
    "        \n",
    "        # Calculate the size after convolutions and pooling\n",
    "        # Input: 187 -> after pool1: 94 -> after pool2: 48 -> after pool3: 25\n",
    "        self.flatten_size = 64 * 25\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(self.flatten_size, 64)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        # MLP (Fully connected layers)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "cnn_model = CNN1D(num_classes=5).to(device)\n",
    "\n",
    "# Print model architecture\n",
    "print(cnn_model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in cnn_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbe900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters configs\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e7ab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# Validation\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch [1/50] - Train Loss: 0.1276, Train Acc: 96.41% - Val Loss: 0.0977, Val Acc: 97.33%\n",
      "Epoch [2/50] - Train Loss: 0.0731, Train Acc: 97.87% - Val Loss: 0.0733, Val Acc: 97.96%\n",
      "Epoch [3/50] - Train Loss: 0.0599, Train Acc: 98.22% - Val Loss: 0.0757, Val Acc: 98.05%\n",
      "Epoch [4/50] - Train Loss: 0.0504, Train Acc: 98.48% - Val Loss: 0.0766, Val Acc: 98.17%\n",
      "Epoch [5/50] - Train Loss: 0.0437, Train Acc: 98.67% - Val Loss: 0.0664, Val Acc: 98.31%\n",
      "Epoch [6/50] - Train Loss: 0.0388, Train Acc: 98.82% - Val Loss: 0.0637, Val Acc: 98.26%\n",
      "Epoch [7/50] - Train Loss: 0.0346, Train Acc: 98.93% - Val Loss: 0.0666, Val Acc: 98.34%\n",
      "Epoch [8/50] - Train Loss: 0.0309, Train Acc: 98.98% - Val Loss: 0.0713, Val Acc: 98.37%\n",
      "Epoch [9/50] - Train Loss: 0.0285, Train Acc: 99.06% - Val Loss: 0.0666, Val Acc: 98.36%\n",
      "Epoch [10/50] - Train Loss: 0.0256, Train Acc: 99.17% - Val Loss: 0.0764, Val Acc: 98.30%\n",
      "Epoch [11/50] - Train Loss: 0.0238, Train Acc: 99.18% - Val Loss: 0.0735, Val Acc: 98.49%\n",
      "Epoch [12/50] - Train Loss: 0.0206, Train Acc: 99.29% - Val Loss: 0.0703, Val Acc: 98.58%\n",
      "Epoch [13/50] - Train Loss: 0.0134, Train Acc: 99.56% - Val Loss: 0.0719, Val Acc: 98.55%\n",
      "Epoch [14/50] - Train Loss: 0.0115, Train Acc: 99.58% - Val Loss: 0.0741, Val Acc: 98.48%\n",
      "Epoch [15/50] - Train Loss: 0.0102, Train Acc: 99.64% - Val Loss: 0.0813, Val Acc: 98.56%\n",
      "Epoch [16/50] - Train Loss: 0.0090, Train Acc: 99.68% - Val Loss: 0.0891, Val Acc: 98.58%\n",
      "\n",
      "Early stopping triggered after 16 epochs\n",
      "\n",
      "Training completed in 267.58 seconds\n",
      "Best model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 50\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "print(\"Starting training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(cnn_model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate_epoch(cnn_model, test_loader, criterion, device)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% - \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save(cnn_model.state_dict(), 'best_cnn_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Load best model\n",
    "cnn_model.load_state_dict(torch.load('best_cnn_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1D CNN Test Accuracy: 98.26%\n",
      "F1 Score (Weighted): 0.9825\n",
      "F1 Score (Macro): 0.9048\n",
      "\n",
      "Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          Normal (N)     0.9902    0.9922    0.9912     18118\n",
      "Supraventricular (S)     0.7870    0.7842    0.7856       556\n",
      "     Ventricular (V)     0.9580    0.9613    0.9597      1448\n",
      "          Fusion (F)     0.8521    0.7469    0.7961       162\n",
      "         Unknown (Q)     0.9969    0.9857    0.9912      1608\n",
      "\n",
      "            accuracy                         0.9826     21892\n",
      "           macro avg     0.9168    0.8941    0.9048     21892\n",
      "        weighted avg     0.9824    0.9826    0.9825     21892\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "cnn_model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = cnn_model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "y_pred_cnn = np.array(all_predictions)\n",
    "y_true_cnn = np.array(all_labels)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true_cnn, y_pred_cnn)\n",
    "f1_weighted = f1_score(y_true_cnn, y_pred_cnn, average='weighted')\n",
    "f1_macro = f1_score(y_true_cnn, y_pred_cnn, average='macro')\n",
    "\n",
    "print(f\"1D CNN Test Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"F1 Score (Weighted): {f1_weighted:.4f}\")\n",
    "print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true_cnn, y_pred_cnn, target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: figures/05_cnn_training_history.pdf\n"
     ]
    }
   ],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "ax1.plot(history['train_acc'], label='Training Accuracy')\n",
    "ax1.plot(history['val_acc'], label='Validation Accuracy')\n",
    "ax1.set_title('1D CNN Model Accuracy (PyTorch)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy (%)')\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss plot\n",
    "ax2.plot(history['train_loss'], label='Training Loss')\n",
    "ax2.plot(history['val_loss'], label='Validation Loss')\n",
    "ax2.set_title('1D CNN Model Loss (PyTorch)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/05_cnn_training_history.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "print(\"Saved: figures/05_cnn_training_history.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "Saved: figures/06_cnn_confusion_matrix.pdf\n"
     ]
    }
   ],
   "source": [
    "# Plot confusion matrix for CNN\n",
    "cm_cnn = confusion_matrix(y_true_cnn, y_pred_cnn)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plot_confusion_matrix(cm_cnn, classes=['N', 'S', 'V', 'F', 'Q'], normalize=True,\n",
    "                      title='1D CNN (PyTorch) - Normalized Confusion Matrix')\n",
    "plt.savefig('figures/06_cnn_confusion_matrix.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "print(\"Saved: figures/06_cnn_confusion_matrix.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baac5c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28d36f76",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e9387d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: np.float64(0.24162492583240192), 1: np.float64(7.877103013945119), 2: np.float64(3.0253628196268143), 3: np.float64(27.317940717628705), 4: np.float64(2.7228735810915876)}\n",
      "Sample weights shape: (87554,)\n"
     ]
    }
   ],
   "source": [
    "# Import XGBoost\n",
    "import xgboost as xgb\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Calculate class weights because of data imbalance\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "sample_weights = np.array([class_weights[label] for label in y_train])\n",
    "\n",
    "print(\"Class weights:\", dict(enumerate(class_weights)))\n",
    "print(f\"Sample weights shape: {sample_weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444ef450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGBoost model...\n",
      "[0]\tvalidation_0-mlogloss:1.47260\n",
      "[1]\tvalidation_0-mlogloss:1.36158\n",
      "[2]\tvalidation_0-mlogloss:1.26792\n",
      "[3]\tvalidation_0-mlogloss:1.18621\n",
      "[4]\tvalidation_0-mlogloss:1.11535\n",
      "[5]\tvalidation_0-mlogloss:1.05069\n",
      "[6]\tvalidation_0-mlogloss:0.99406\n",
      "[7]\tvalidation_0-mlogloss:0.94285\n",
      "[8]\tvalidation_0-mlogloss:0.89505\n",
      "[9]\tvalidation_0-mlogloss:0.85095\n",
      "[10]\tvalidation_0-mlogloss:0.81165\n",
      "[11]\tvalidation_0-mlogloss:0.77408\n",
      "[12]\tvalidation_0-mlogloss:0.74108\n",
      "[13]\tvalidation_0-mlogloss:0.71043\n",
      "[14]\tvalidation_0-mlogloss:0.68204\n",
      "[15]\tvalidation_0-mlogloss:0.65567\n",
      "[16]\tvalidation_0-mlogloss:0.63193\n",
      "[17]\tvalidation_0-mlogloss:0.60982\n",
      "[18]\tvalidation_0-mlogloss:0.58806\n",
      "[19]\tvalidation_0-mlogloss:0.56944\n",
      "[20]\tvalidation_0-mlogloss:0.55087\n",
      "[21]\tvalidation_0-mlogloss:0.53362\n",
      "[22]\tvalidation_0-mlogloss:0.51803\n",
      "[23]\tvalidation_0-mlogloss:0.50379\n",
      "[24]\tvalidation_0-mlogloss:0.49070\n",
      "[25]\tvalidation_0-mlogloss:0.47719\n",
      "[26]\tvalidation_0-mlogloss:0.46493\n",
      "[27]\tvalidation_0-mlogloss:0.45388\n",
      "[28]\tvalidation_0-mlogloss:0.44365\n",
      "[29]\tvalidation_0-mlogloss:0.43295\n",
      "[30]\tvalidation_0-mlogloss:0.42305\n",
      "[31]\tvalidation_0-mlogloss:0.41435\n",
      "[32]\tvalidation_0-mlogloss:0.40626\n",
      "[33]\tvalidation_0-mlogloss:0.39795\n",
      "[34]\tvalidation_0-mlogloss:0.39097\n",
      "[35]\tvalidation_0-mlogloss:0.38329\n",
      "[36]\tvalidation_0-mlogloss:0.37677\n",
      "[37]\tvalidation_0-mlogloss:0.36988\n",
      "[38]\tvalidation_0-mlogloss:0.36451\n",
      "[39]\tvalidation_0-mlogloss:0.35880\n",
      "[40]\tvalidation_0-mlogloss:0.35358\n",
      "[41]\tvalidation_0-mlogloss:0.34825\n",
      "[42]\tvalidation_0-mlogloss:0.34400\n",
      "[43]\tvalidation_0-mlogloss:0.33795\n",
      "[44]\tvalidation_0-mlogloss:0.33383\n",
      "[45]\tvalidation_0-mlogloss:0.32851\n",
      "[46]\tvalidation_0-mlogloss:0.32400\n",
      "[47]\tvalidation_0-mlogloss:0.31942\n",
      "[48]\tvalidation_0-mlogloss:0.31398\n",
      "[49]\tvalidation_0-mlogloss:0.30935\n",
      "[50]\tvalidation_0-mlogloss:0.30565\n",
      "[51]\tvalidation_0-mlogloss:0.30142\n",
      "[52]\tvalidation_0-mlogloss:0.29704\n",
      "[53]\tvalidation_0-mlogloss:0.29323\n",
      "[54]\tvalidation_0-mlogloss:0.29033\n",
      "[55]\tvalidation_0-mlogloss:0.28681\n",
      "[56]\tvalidation_0-mlogloss:0.28403\n",
      "[57]\tvalidation_0-mlogloss:0.28094\n",
      "[58]\tvalidation_0-mlogloss:0.27783\n",
      "[59]\tvalidation_0-mlogloss:0.27516\n",
      "[60]\tvalidation_0-mlogloss:0.27263\n",
      "[61]\tvalidation_0-mlogloss:0.27013\n",
      "[62]\tvalidation_0-mlogloss:0.26722\n",
      "[63]\tvalidation_0-mlogloss:0.26473\n",
      "[64]\tvalidation_0-mlogloss:0.26226\n",
      "[65]\tvalidation_0-mlogloss:0.25951\n",
      "[66]\tvalidation_0-mlogloss:0.25677\n",
      "[67]\tvalidation_0-mlogloss:0.25419\n",
      "[68]\tvalidation_0-mlogloss:0.25161\n",
      "[69]\tvalidation_0-mlogloss:0.24922\n",
      "[70]\tvalidation_0-mlogloss:0.24699\n",
      "[71]\tvalidation_0-mlogloss:0.24451\n",
      "[72]\tvalidation_0-mlogloss:0.24200\n",
      "[73]\tvalidation_0-mlogloss:0.23941\n",
      "[74]\tvalidation_0-mlogloss:0.23733\n",
      "[75]\tvalidation_0-mlogloss:0.23480\n",
      "[76]\tvalidation_0-mlogloss:0.23260\n",
      "[77]\tvalidation_0-mlogloss:0.23032\n",
      "[78]\tvalidation_0-mlogloss:0.22870\n",
      "[79]\tvalidation_0-mlogloss:0.22672\n",
      "[80]\tvalidation_0-mlogloss:0.22436\n",
      "[81]\tvalidation_0-mlogloss:0.22214\n",
      "[82]\tvalidation_0-mlogloss:0.22030\n",
      "[83]\tvalidation_0-mlogloss:0.21855\n",
      "[84]\tvalidation_0-mlogloss:0.21739\n",
      "[85]\tvalidation_0-mlogloss:0.21543\n",
      "[86]\tvalidation_0-mlogloss:0.21376\n",
      "[87]\tvalidation_0-mlogloss:0.21173\n",
      "[88]\tvalidation_0-mlogloss:0.20986\n",
      "[89]\tvalidation_0-mlogloss:0.20866\n",
      "[90]\tvalidation_0-mlogloss:0.20630\n",
      "[91]\tvalidation_0-mlogloss:0.20453\n",
      "[92]\tvalidation_0-mlogloss:0.20254\n",
      "[93]\tvalidation_0-mlogloss:0.20080\n",
      "[94]\tvalidation_0-mlogloss:0.19957\n",
      "[95]\tvalidation_0-mlogloss:0.19778\n",
      "[96]\tvalidation_0-mlogloss:0.19628\n",
      "[97]\tvalidation_0-mlogloss:0.19464\n",
      "[98]\tvalidation_0-mlogloss:0.19339\n",
      "[99]\tvalidation_0-mlogloss:0.19194\n",
      "\n",
      "Training completed in 33.19 seconds\n"
     ]
    }
   ],
   "source": [
    "# Initialize XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    objective='multi:softmax',\n",
    "    num_class=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric='mlogloss',\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    sample_weight=sample_weights,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d0f874eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Test Accuracy: 94.78%\n",
      "F1 Score (Weighted): 0.9528\n",
      "F1 Score (Macro): 0.8010\n",
      "\n",
      "Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          Normal (N)     0.9902    0.9502    0.9698     18118\n",
      "Supraventricular (S)     0.4902    0.8129    0.6116       556\n",
      "     Ventricular (V)     0.8829    0.9427    0.9118      1448\n",
      "          Fusion (F)     0.4051    0.8827    0.5553       162\n",
      "         Unknown (Q)     0.9347    0.9789    0.9563      1608\n",
      "\n",
      "            accuracy                         0.9478     21892\n",
      "           macro avg     0.7406    0.9135    0.8010     21892\n",
      "        weighted avg     0.9620    0.9478    0.9528     21892\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_xgb)\n",
    "f1_weighted = f1_score(y_test, y_pred_xgb, average='weighted')\n",
    "f1_macro = f1_score(y_test, y_pred_xgb, average='macro')\n",
    "\n",
    "print(f\"XGBoost Test Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"F1 Score (Weighted): {f1_weighted:.4f}\")\n",
    "print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2810bada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "Saved: figures/08_xgb_confusion_matrix.pdf\n"
     ]
    }
   ],
   "source": [
    "# Plot confusion matrix for XGBoost\n",
    "cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plot_confusion_matrix(cm_xgb, classes=['N', 'S', 'V', 'F', 'Q'], normalize=True,\n",
    "                      title='XGBoost - Normalized Confusion Matrix')\n",
    "plt.savefig('figures/08_xgb_confusion_matrix.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "print(\"Saved: figures/08_xgb_confusion_matrix.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb3498e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec747e30",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cb479dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_lstm shape: torch.Size([87554, 187, 1])\n",
      "X_test_lstm shape: torch.Size([21892, 187, 1])\n",
      "Number of LSTM training batches: 2737\n",
      "Number of LSTM test batches: 685\n"
     ]
    }
   ],
   "source": [
    "# LSTM expects input shape: (batch_size, sequence_length, input_size)\n",
    "# For ECG: sequence_length=187, input_size=1\n",
    "\n",
    "X_train_lstm = torch.FloatTensor(X_train_scaled).unsqueeze(2)  # Shape: (N, 187, 1)\n",
    "X_test_lstm = torch.FloatTensor(X_test_scaled).unsqueeze(2)\n",
    "\n",
    "print(f\"X_train_lstm shape: {X_train_lstm.shape}\")\n",
    "print(f\"X_test_lstm shape: {X_test_lstm.shape}\")\n",
    "\n",
    "# Create DataLoaders for LSTM\n",
    "lstm_train_dataset = TensorDataset(X_train_lstm, y_train_tensor)\n",
    "lstm_test_dataset = TensorDataset(X_test_lstm, y_test_tensor)\n",
    "\n",
    "lstm_train_loader = DataLoader(lstm_train_dataset, batch_size=32, shuffle=True)\n",
    "lstm_test_loader = DataLoader(lstm_test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"LSTM training batches: {len(lstm_train_loader)}\")\n",
    "print(f\"LSTM test batches: {len(lstm_test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d77cc61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (lstm): LSTM(1, 128, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc2): Linear(in_features=64, out_features=5, bias=True)\n",
      ")\n",
      "\n",
      "Total parameters: 207,749\n"
     ]
    }
   ],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=128, num_layers=2, num_classes=5, dropout=0.3):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_size, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length, input_size)\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        # lstm_out shape: (batch_size, sequence_length, hidden_size)\n",
    "        # h_n and c_n shape: (num_layers, batch_size, hidden_size)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        \n",
    "        # Take the output from the last time step\n",
    "        # out shape: (batch_size, hidden_size)\n",
    "        out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Fully connected layers\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Initialize LSTM model\n",
    "lstm_model = LSTMModel(\n",
    "    input_size=1,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    num_classes=5,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "# Print model architecture\n",
    "print(lstm_model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in lstm_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4c1d870d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM training setup completed.\n"
     ]
    }
   ],
   "source": [
    "lstm_criterion = nn.CrossEntropyLoss()\n",
    "lstm_optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "lstm_scheduler = optim.lr_scheduler.ReduceLROnPlateau(lstm_optimizer, mode='min', factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LSTM training...\n",
      "Epoch [1/50] - Train Loss: 0.6453, Train Acc: 82.74% - Val Loss: 0.5971, Val Acc: 82.76%\n",
      "Epoch [2/50] - Train Loss: 0.5534, Train Acc: 82.73% - Val Loss: 0.5234, Val Acc: 83.54%\n",
      "Epoch [3/50] - Train Loss: 0.5368, Train Acc: 83.52% - Val Loss: 0.5420, Val Acc: 84.09%\n",
      "Epoch [4/50] - Train Loss: 0.4778, Train Acc: 86.08% - Val Loss: 0.3274, Val Acc: 91.43%\n",
      "Epoch [5/50] - Train Loss: 0.2857, Train Acc: 92.37% - Val Loss: 0.2408, Val Acc: 93.84%\n",
      "Epoch [6/50] - Train Loss: 0.2229, Train Acc: 93.94% - Val Loss: 0.2138, Val Acc: 93.98%\n",
      "Epoch [7/50] - Train Loss: 0.1893, Train Acc: 94.75% - Val Loss: 0.1603, Val Acc: 95.61%\n",
      "Epoch [8/50] - Train Loss: 0.2236, Train Acc: 93.83% - Val Loss: 0.2044, Val Acc: 94.19%\n",
      "Epoch [9/50] - Train Loss: 0.2204, Train Acc: 93.88% - Val Loss: 0.2097, Val Acc: 93.55%\n",
      "Epoch [10/50] - Train Loss: 0.1706, Train Acc: 95.14% - Val Loss: 0.1707, Val Acc: 95.07%\n",
      "Epoch [11/50] - Train Loss: 0.1509, Train Acc: 95.67% - Val Loss: 0.1462, Val Acc: 95.95%\n",
      "Epoch [12/50] - Train Loss: 0.1371, Train Acc: 96.18% - Val Loss: 0.1377, Val Acc: 95.87%\n",
      "Epoch [13/50] - Train Loss: 0.1285, Train Acc: 96.38% - Val Loss: 0.1227, Val Acc: 96.62%\n",
      "Epoch [14/50] - Train Loss: 0.1186, Train Acc: 96.70% - Val Loss: 0.1502, Val Acc: 95.56%\n",
      "Epoch [15/50] - Train Loss: 0.6151, Train Acc: 83.92% - Val Loss: 0.6454, Val Acc: 83.06%\n",
      "Epoch [16/50] - Train Loss: 0.6383, Train Acc: 83.18% - Val Loss: 0.6451, Val Acc: 82.79%\n",
      "Epoch [17/50] - Train Loss: 0.6146, Train Acc: 83.55% - Val Loss: 0.4286, Val Acc: 86.44%\n",
      "Epoch [18/50] - Train Loss: 0.3722, Train Acc: 89.08% - Val Loss: 0.2881, Val Acc: 91.90%\n",
      "Epoch [19/50] - Train Loss: 0.2781, Train Acc: 91.89% - Val Loss: 0.2363, Val Acc: 93.24%\n",
      "Epoch [20/50] - Train Loss: 0.4220, Train Acc: 88.69% - Val Loss: 0.2917, Val Acc: 92.82%\n",
      "Epoch [21/50] - Train Loss: 0.3421, Train Acc: 91.02% - Val Loss: 0.2753, Val Acc: 93.18%\n",
      "Epoch [22/50] - Train Loss: 0.3429, Train Acc: 91.15% - Val Loss: 0.3397, Val Acc: 91.28%\n",
      "Epoch [23/50] - Train Loss: 0.3084, Train Acc: 91.98% - Val Loss: 0.2704, Val Acc: 93.22%\n",
      "\n",
      "Early stopping triggered after 23 epochs\n",
      "\n",
      "LSTM training completed in 702.22 seconds\n",
      "Best LSTM model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Train LSTM model\n",
    "num_epochs_lstm = 50\n",
    "patience_lstm = 10\n",
    "best_val_loss_lstm = float('inf')\n",
    "patience_counter_lstm = 0\n",
    "\n",
    "history_lstm = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "print(\"Starting LSTM training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs_lstm):\n",
    "    train_loss, train_acc = train_epoch(lstm_model, lstm_train_loader, lstm_criterion, lstm_optimizer, device)\n",
    "    val_loss, val_acc = validate_epoch(lstm_model, lstm_test_loader, lstm_criterion, device)\n",
    "    \n",
    "    history_lstm['train_loss'].append(train_loss)\n",
    "    history_lstm['train_acc'].append(train_acc)\n",
    "    history_lstm['val_loss'].append(val_loss)\n",
    "    history_lstm['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs_lstm}] - \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% - \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    lstm_scheduler.step(val_loss)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss_lstm:\n",
    "        best_val_loss_lstm = val_loss\n",
    "        patience_counter_lstm = 0\n",
    "        # Save best model\n",
    "        torch.save(lstm_model.state_dict(), 'best_lstm_model.pth')\n",
    "    else:\n",
    "        patience_counter_lstm += 1\n",
    "        if patience_counter_lstm >= patience_lstm:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "\n",
    "training_time_lstm = time.time() - start_time\n",
    "print(f\"\\nLSTM training completed in {training_time_lstm:.2f} seconds\")\n",
    "\n",
    "# Load best model\n",
    "lstm_model.load_state_dict(torch.load('best_lstm_model.pth'))\n",
    "print(\"Best LSTM model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Test Accuracy: 96.62%\n",
      "F1 Score (Weighted): 0.9638\n",
      "F1 Score (Macro): 0.8144\n",
      "\n",
      "Classification Report:\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "          Normal (N)     0.9714    0.9935    0.9823     18118\n",
      "Supraventricular (S)     0.8511    0.5036    0.6328       556\n",
      "     Ventricular (V)     0.9219    0.8798    0.9004      1448\n",
      "          Fusion (F)     0.7034    0.5123    0.5929       162\n",
      "         Unknown (Q)     0.9876    0.9409    0.9637      1608\n",
      "\n",
      "            accuracy                         0.9662     21892\n",
      "           macro avg     0.8871    0.7660    0.8144     21892\n",
      "        weighted avg     0.9643    0.9662    0.9638     21892\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate LSTM model\n",
    "lstm_model.eval()\n",
    "all_predictions_lstm = []\n",
    "all_labels_lstm = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in lstm_test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = lstm_model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        all_predictions_lstm.extend(predicted.cpu().numpy())\n",
    "        all_labels_lstm.extend(labels.numpy())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "y_pred_lstm = np.array(all_predictions_lstm)\n",
    "y_true_lstm = np.array(all_labels_lstm)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_lstm = accuracy_score(y_true_lstm, y_pred_lstm)\n",
    "f1_weighted_lstm = f1_score(y_true_lstm, y_pred_lstm, average='weighted')\n",
    "f1_macro_lstm = f1_score(y_true_lstm, y_pred_lstm, average='macro')\n",
    "\n",
    "print(f\"LSTM Test Accuracy: {accuracy_lstm*100:.2f}%\")\n",
    "print(f\"F1 Score (Weighted): {f1_weighted_lstm:.4f}\")\n",
    "print(f\"F1 Score (Macro): {f1_macro_lstm:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true_lstm, y_pred_lstm, target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1904deb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: figures/09_lstm_training_history.pdf\n"
     ]
    }
   ],
   "source": [
    "# Plot LSTM training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "ax1.plot(history_lstm['train_acc'], label='Training Accuracy')\n",
    "ax1.plot(history_lstm['val_acc'], label='Validation Accuracy')\n",
    "ax1.set_title('LSTM Model Accuracy (PyTorch)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy (%)')\n",
    "ax1.legend(loc='lower right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss plot\n",
    "ax2.plot(history_lstm['train_loss'], label='Training Loss')\n",
    "ax2.plot(history_lstm['val_loss'], label='Validation Loss')\n",
    "ax2.set_title('LSTM Model Loss (PyTorch)', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/09_lstm_training_history.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "print(\"Saved: figures/09_lstm_training_history.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1f1ba234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "Saved: figures/10_lstm_confusion_matrix.pdf\n"
     ]
    }
   ],
   "source": [
    "# Plot confusion matrix for LSTM\n",
    "cm_lstm = confusion_matrix(y_true_lstm, y_pred_lstm)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plot_confusion_matrix(cm_lstm, classes=['N', 'S', 'V', 'F', 'Q'], normalize=True,\n",
    "                      title='LSTM (PyTorch) - Normalized Confusion Matrix')\n",
    "plt.savefig('figures/10_lstm_confusion_matrix.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "print(\"Saved: figures/10_lstm_confusion_matrix.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68f5179",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6dee4fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Model Comparison:\n",
      "================================================================================\n",
      "        Model  Accuracy (%)  F1 (Weighted)  F1 (Macro)\n",
      "Random Forest     97.446556       0.972790    0.871586\n",
      "      XGBoost     94.783483       0.952790    0.800967\n",
      "       1D CNN     98.255070       0.982456    0.904751\n",
      "         LSTM     96.615202       0.963792    0.814404\n",
      "================================================================================\n",
      "\n",
      "Best performing model: 1D CNN with 98.26% accuracy\n"
     ]
    }
   ],
   "source": [
    "# Compare all models\n",
    "models_comparison = {\n",
    "    'Model': ['Random Forest', 'XGBoost', '1D CNN', 'LSTM'],\n",
    "    'Accuracy (%)': [\n",
    "        accuracy_score(y_test, y_pred_rf) * 100,\n",
    "        accuracy_score(y_test, y_pred_xgb) * 100,\n",
    "        accuracy_score(y_true_cnn, y_pred_cnn) * 100,\n",
    "        accuracy_score(y_true_lstm, y_pred_lstm) * 100\n",
    "    ],\n",
    "    'F1 (Weighted)': [\n",
    "        f1_score(y_test, y_pred_rf, average='weighted'),\n",
    "        f1_score(y_test, y_pred_xgb, average='weighted'),\n",
    "        f1_score(y_true_cnn, y_pred_cnn, average='weighted'),\n",
    "        f1_score(y_true_lstm, y_pred_lstm, average='weighted')\n",
    "    ],\n",
    "    'F1 (Macro)': [\n",
    "        f1_score(y_test, y_pred_rf, average='macro'),\n",
    "        f1_score(y_test, y_pred_xgb, average='macro'),\n",
    "        f1_score(y_true_cnn, y_pred_cnn, average='macro'),\n",
    "        f1_score(y_true_lstm, y_pred_lstm, average='macro')\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(models_comparison)\n",
    "print(\"\\nFinal Model Comparison:\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find best model\n",
    "best_idx = comparison_df['Accuracy (%)'].idxmax()\n",
    "print(f\"\\nBest performing model: {comparison_df.loc[best_idx, 'Model']} \"\n",
    "      f\"with {comparison_df.loc[best_idx, 'Accuracy (%)']:.2f}% accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890c9cc7",
   "metadata": {},
   "source": [
    "## Generated PDF Figures for LaTeX\n",
    "\n",
    "All plots have been saved as PDF files in the `figures/` directory. Use them in your LaTeX document with:\n",
    "\n",
    "```latex\n",
    "\\begin{figure}[h]\n",
    "    \\centering\n",
    "    \\includegraphics[width=0.8\\textwidth]{figures/filename.pdf}\n",
    "    \\caption{Your caption here}\n",
    "    \\label{fig:your_label}\n",
    "\\end{figure}\n",
    "```\n",
    "\n",
    "### List of Generated Figures:\n",
    "\n",
    "1. **01_class_distribution.pdf** - Bar chart and pie chart showing class distribution\n",
    "2. **02_ecg_signals_per_class.pdf** - Sample ECG signals from each class\n",
    "3. **03_rf_feature_importance.pdf** - Random Forest feature importance (top 20)\n",
    "4. **04_rf_confusion_matrix.pdf** - Random Forest normalized confusion matrix\n",
    "5. **05_cnn_training_history.pdf** - 1D CNN training accuracy and loss curves\n",
    "6. **06_cnn_confusion_matrix.pdf** - 1D CNN normalized confusion matrix\n",
    "7. **07_xgb_feature_importance.pdf** - XGBoost feature importance (top 20)\n",
    "8. **08_xgb_confusion_matrix.pdf** - XGBoost normalized confusion matrix\n",
    "9. **09_lstm_training_history.pdf** - LSTM training accuracy and loss curves\n",
    "10. **10_lstm_confusion_matrix.pdf** - LSTM normalized confusion matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
